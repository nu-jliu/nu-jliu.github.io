---
layout: post
title: "Vision-Language-Action Models for Robotic Manipulation with SoArm-101"
categories: [Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning]
image: assets/images/vla_placeholder.gif
featured: false
hidden: false
---

Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning

[View This Project on GitHub](https://github.com/nu-jliu/allen-vla)

# Description

This project explores the application of state-of-the-art Vision-Language-Action (VLA) models for robotic manipulation tasks using the SoArm-101 robot arm. VLA models represent a new paradigm in robot learning that combines visual perception, natural language understanding, and action prediction to enable robots to perform complex manipulation tasks through learned behaviors.

The project is structured around two primary objectives:

1. **Teleoperation & Data Collection**: Establishing a robust pipeline for teleoperating the SoArm-101 robot and collecting high-quality demonstration datasets using LeRobot
2. **Training & Deployment**: Implementing, training, and benchmarking various VLA model architectures including π0, π0.5, and ACT (Action Chunking Transformer)

# Project Overview

## What are Vision-Language-Action Models?

Vision-Language-Action (VLA) models are a class of neural networks that learn to map visual observations and language instructions directly to robot actions. Unlike traditional robotic systems that rely on hand-engineered perception and control pipelines, VLA models learn end-to-end policies from demonstration data, enabling more flexible and generalizable robot behaviors.

These models typically:
- Process camera images to understand the scene
- Accept natural language commands to specify task goals
- Output low-level robot control commands (joint positions, velocities, etc.)

## Hardware Setup

The project uses the **SoArm-101** robot arm, a compact and accessible robotic manipulator suitable for tabletop manipulation tasks. The system is integrated with LeRobot, an open-source framework designed to facilitate robot learning research.

Key components:
- **SoArm-101 Robot Arm**: 5-DOF robotic arm with gripper
- **Camera System**: Vision sensors for visual perception
- **Teleoperation Interface**: Custom control interface for collecting demonstration data
- **LeRobot Framework**: Provides tools for data collection, training, and deployment

# Project Phases

## Phase 1: Teleoperation & Data Collection Pipeline

The first phase focuses on establishing a reliable data collection infrastructure. High-quality demonstration data is crucial for training effective VLA models.

### Completed Tasks:
- **LeRobot Environment Setup**: Configured LeRobot framework with all necessary dependencies
- **Hardware Integration**: Successfully integrated SoArm-101 with LeRobot's control interfaces
- **Teleoperation Interface**: Developed an intuitive teleoperation system allowing human operators to control the robot
- **Data Collection Scripts**: Implemented automated scripts for recording and storing demonstration trajectories
- **Sample Demonstrations**: Tested the complete pipeline by recording initial demonstration datasets

### Ongoing Work:
- **Camera & Sensor Pipeline**: Finalizing the vision system configuration for optimal data quality
- **Data Format Standardization**: Defining consistent data structures for training compatibility
- **Dataset Management**: Creating utilities for organizing, versioning, and managing collected datasets

## Phase 2: Training & Deployment Pipeline

The second phase involves implementing and evaluating different VLA model architectures.

### Planned Models:

#### π0 (Pi-Zero)
A foundational VLA model architecture that learns visuomotor policies from demonstration data.

#### π0.5 (Pi-Zero-Point-Five)
An enhanced version of π0 with improved architecture and training strategies.

#### ACT (Action Chunking Transformer)
A transformer-based model that predicts sequences of future actions (action chunks) rather than single actions, enabling more coherent and long-horizon behaviors.

### Training Pipeline Components:
- GPU-accelerated training environment
- Hyperparameter configuration management
- Model checkpointing and versioning
- Training visualization and monitoring
- Evaluation metrics and benchmarking scripts

### Deployment Pipeline:
- Model inference optimization for real-time control
- Hardware deployment interface
- Safety monitoring and intervention systems
- Performance evaluation on real hardware

# Technical Implementation

## LeRobot Integration

LeRobot provides a standardized interface for robot learning research. The integration with SoArm-101 required:

1. **Robot Driver Development**: Custom drivers for SoArm-101 hardware communication
2. **Calibration Procedures**: Kinematic calibration for accurate position control
3. **Data Recording Format**: Standardized format compatible with LeRobot's training pipeline
4. **Real-time Control**: Low-latency control loops for smooth teleoperation and policy execution

## Data Collection Methodology

The teleoperation system enables human operators to:
- Control the robot arm through an intuitive interface
- Demonstrate desired manipulation behaviors
- Record synchronized data streams (images, robot states, actions)
- Annotate demonstrations with task labels and language descriptions

The collected datasets serve as training data for the VLA models, where the models learn to imitate the demonstrated behaviors.

## Model Training Workflow

The planned training workflow follows these steps:

1. **Data Preprocessing**: Normalize images and actions, augment data for robustness
2. **Model Architecture**: Implement the selected VLA model architecture
3. **Training Loop**: Optimize model parameters using collected demonstrations
4. **Validation**: Evaluate model performance on held-out demonstrations
5. **Deployment Testing**: Test trained policies on the physical robot
6. **Iteration**: Refine based on real-world performance

# Current Status

The project has successfully completed Phase 1 milestones related to teleoperation and basic data collection infrastructure. The system is now capable of:

- Teleoperating the SoArm-101 with low latency
- Recording demonstration data in LeRobot-compatible format
- Managing collected datasets

Phase 2 development is underway, focusing on establishing the training infrastructure and implementing the first VLA model architectures.

# Future Directions

This project aims to:

1. **Benchmark Multiple VLA Architectures**: Compare π0, π0.5, ACT, and potentially other models on standardized tasks
2. **Dataset Expansion**: Collect diverse demonstration datasets covering various manipulation scenarios
3. **Performance Analysis**: Document detailed performance metrics, failure modes, and lessons learned
4. **Open-Source Contributions**: Share trained models, datasets, and implementation insights with the robotics community

# Technical Stack

- **Programming Language**: Python 3.12+
- **Robotics Framework**: ROS2
- **Robot Learning Framework**: LeRobot (v0.4.2+)
- **Hardware Interface**: Feetech servos
- **Deep Learning**: PyTorch (via LeRobot)
- **Version Control**: Git

# Conclusion

This project represents an exploration of cutting-edge VLA models applied to practical robotic manipulation. By combining teleoperation, imitation learning, and state-of-the-art neural architectures, the goal is to develop capable and generalizable robot policies that can learn from demonstration and adapt to new scenarios.

The integration of LeRobot with SoArm-101 provides an accessible platform for robot learning research, and the insights gained from training and evaluating multiple VLA models will contribute to understanding the capabilities and limitations of these emerging approaches.

# References

1. LeRobot: An Open-Source Framework for Robot Learning - [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)
2. Action Chunking Transformer (ACT) - Learning Fine-Grained Bimanual Manipulation
3. Vision-Language-Action Models for Robot Learning
