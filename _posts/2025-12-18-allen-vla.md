---
layout: post
title: "Vision-Language-Action Models for Robotic Manipulation with SoArm-101"
categories: [Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning]
image: assets/images/vla_placeholder.gif
featured: false
hidden: false
---

Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning

[View This Project on GitHub](https://github.com/nu-jliu/allen-vla)

# Description

This project explores the application of state-of-the-art Vision-Language-Action (VLA) models for robotic manipulation tasks using the SoArm-101 robot arm. VLA models represent a new paradigm in robot learning that combines visual perception, natural language understanding, and action prediction to enable robots to perform complex manipulation tasks through learned behaviors.

The project is structured around two primary objectives:

1. **Teleoperation & Data Collection**: Establishing a robust pipeline for teleoperating the SoArm-101 robot and collecting high-quality demonstration datasets using LeRobot
2. **Training & Deployment**: Implementing, training, and benchmarking various VLA model architectures including π0, π0.5, and ACT (Action Chunking Transformer)

# Project Overview

## What are Vision-Language-Action Models?

Vision-Language-Action (VLA) models are a class of neural networks that learn to map visual observations and language instructions directly to robot actions. Unlike traditional robotic systems that rely on hand-engineered perception and control pipelines, VLA models learn end-to-end policies from demonstration data, enabling more flexible and generalizable robot behaviors.

These models typically:
- Process camera images to understand the scene
- Accept natural language commands to specify task goals
- Output low-level robot control commands (joint positions, velocities, etc.)

## Hardware Setup

The project uses the **SoArm-101** robot arm, a compact and accessible robotic manipulator suitable for tabletop manipulation tasks. The system is integrated with LeRobot, an open-source framework designed to facilitate robot learning research.

Key components:
- **SoArm-101 Robot Arm**: 5-DOF robotic arm with gripper
- **Camera System**: Vision sensors for visual perception
- **Teleoperation Interface**: Custom control interface for collecting demonstration data
- **LeRobot Framework**: Provides tools for data collection, training, and deployment

# Project Phases

## Phase 1: Teleoperation & Data Collection Pipeline ✓ COMPLETE

The first phase focused on establishing a reliable data collection infrastructure. High-quality demonstration data is crucial for training effective VLA models. **This phase has been successfully completed.**

### Completed Tasks:
- **LeRobot Environment Setup**: Configured LeRobot framework with all necessary dependencies
- **Hardware Integration**: Successfully integrated SoArm-101 with LeRobot's control interfaces
- **Teleoperation Interface**: Developed an intuitive leader-follower teleoperation system where movements on a leader arm transmit to a follower arm in real-time
- **Data Collection Scripts**: Implemented automated scripts (`data_collection.py`) for recording and storing demonstration trajectories at 30 Hz
- **Sample Demonstrations**: Tested the complete pipeline by recording initial demonstration datasets
- **Camera & Sensor Pipeline**: Completed vision system configuration and integration
- **Data Format Standardization**: Implemented consistent data structures compatible with LeRobot's training format
- **Dataset Management**: Created utilities for organizing, versioning, and managing collected datasets with Hugging Face Hub integration

## Phase 2: Training & Deployment Pipeline

The second phase involves implementing and evaluating different VLA model architectures.

### Planned Models:

#### π0 (Pi-Zero)
A foundational VLA model architecture that learns visuomotor policies from demonstration data.

#### π0.5 (Pi-Zero-Point-Five)
An enhanced version of π0 with improved architecture and training strategies.

#### ACT (Action Chunking Transformer)
A transformer-based model that predicts sequences of future actions (action chunks) rather than single actions, enabling more coherent and long-horizon behaviors.

### Training Pipeline Components:
- GPU-accelerated training environment
- Hyperparameter configuration management
- Model checkpointing and versioning
- Training visualization and monitoring
- Evaluation metrics and benchmarking scripts

### Deployment Pipeline:
- Model inference optimization for real-time control
- Hardware deployment interface
- Safety monitoring and intervention systems
- Performance evaluation on real hardware

# Technical Implementation

## LeRobot Integration

LeRobot provides a standardized interface for robot learning research. The integration with SoArm-101 required:

1. **Robot Driver Development**: Custom drivers for SoArm-101 hardware communication
2. **Calibration Procedures**: Kinematic calibration for accurate position control
3. **Data Recording Format**: Standardized format compatible with LeRobot's training pipeline
4. **Real-time Control**: Low-latency control loops for smooth teleoperation and policy execution

## Data Collection Methodology

The teleoperation system enables human operators to:
- Control the robot arm through an intuitive interface
- Demonstrate desired manipulation behaviors
- Record synchronized data streams (images, robot states, actions)
- Annotate demonstrations with task labels and language descriptions

The collected datasets serve as training data for the VLA models, where the models learn to imitate the demonstrated behaviors.

## Usage Workflows

The project provides two main operational modes:

### Teleoperation Mode

Running `python teleop.py` establishes a leader-follower control system:
- Movements on the leader arm are transmitted to the follower arm in real-time
- Serial port configuration and arm identifiers are customizable via command-line parameters
- Enables intuitive control for demonstrating manipulation behaviors

### Data Collection Mode

The `data_collection.py` script captures demonstration episodes:
- Records synchronized data streams at 30 Hz (images, robot states, actions)
- Press ENTER to toggle recording on/off
- Press CTRL+C to finalize the session and upload datasets to Hugging Face Hub
- Datasets are automatically formatted for compatibility with LeRobot's training pipeline

## Model Training Workflow

The planned training workflow follows these steps:

1. **Data Preprocessing**: Normalize images and actions, augment data for robustness
2. **Model Architecture**: Implement the selected VLA model architecture
3. **Training Loop**: Optimize model parameters using collected demonstrations
4. **Validation**: Evaluate model performance on held-out demonstrations
5. **Deployment Testing**: Test trained policies on the physical robot
6. **Iteration**: Refine based on real-world performance

# Current Status

The project has successfully completed Phase 1 milestones related to teleoperation and basic data collection infrastructure. The system is now capable of:

- Teleoperating the SoArm-101 with low latency
- Recording demonstration data in LeRobot-compatible format
- Managing collected datasets

Phase 2 development is underway, focusing on establishing the training infrastructure and implementing the first VLA model architectures.

# Future Directions

This project aims to:

1. **Benchmark Multiple VLA Architectures**: Compare π0, π0.5, ACT, and potentially other models on standardized tasks
2. **Dataset Expansion**: Collect diverse demonstration datasets covering various manipulation scenarios
3. **Performance Analysis**: Document detailed performance metrics, failure modes, and lessons learned
4. **Open-Source Contributions**: Share trained models, datasets, and implementation insights with the robotics community

# Technical Stack

- **Programming Language**: Python 3.12+
- **Dependency Management**: UV
- **Robotics Framework**: ROS2
- **Robot Learning Framework**: LeRobot (v0.4.2+)
- **Hardware Interface**: Feetech servos
- **Deep Learning**: PyTorch (via LeRobot)
- **Dataset Management**: Hugging Face Hub
- **Version Control**: Git

# Conclusion

This project represents an exploration of cutting-edge VLA models applied to practical robotic manipulation. By combining teleoperation, imitation learning, and state-of-the-art neural architectures, the goal is to develop capable and generalizable robot policies that can learn from demonstration and adapt to new scenarios.

The integration of LeRobot with SoArm-101 provides an accessible platform for robot learning research, and the insights gained from training and evaluating multiple VLA models will contribute to understanding the capabilities and limitations of these emerging approaches.

# License

This project is released under the MIT License, enabling broad usage and modification with proper attribution. The open-source nature of the project encourages collaboration and knowledge sharing within the robotics and machine learning communities.

# References

1. LeRobot: An Open-Source Framework for Robot Learning - [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)
2. Action Chunking Transformer (ACT) - Learning Fine-Grained Bimanual Manipulation
3. Vision-Language-Action Models for Robot Learning
