---
layout: post
title: "Vision-Language-Action Models for Robotic Manipulation with SoArm-101"
categories: [Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning]
image: assets/images/vla_demo.gif
featured: false
hidden: false
---

Python, ROS2, LeRobot, Vision-Language-Action, Deep Learning, Robot Manipulation, SoArm-101, Teleoperation, Imitation Learning

[View This Project on GitHub](https://github.com/nu-jliu/allen-vla)

# Description

This project explores the application of state-of-the-art Vision-Language-Action (VLA) models for robotic manipulation tasks using the SoArm-101 robot arm. VLA models represent a new paradigm in robot learning that combines visual perception, natural language understanding, and action prediction to enable robots to perform complex manipulation tasks through learned behaviors.

The project is structured around two primary objectives:

1. **Teleoperation & Data Collection**: Establishing a robust pipeline for teleoperating the SoArm-101 robot and collecting high-quality demonstration datasets using LeRobot
2. **Training & Deployment**: Implementing, training, and benchmarking various VLA model architectures including π0, π0.5, and ACT (Action Chunking Transformer)

# Project Overview

## What are Vision-Language-Action Models?

Vision-Language-Action (VLA) models are a class of neural networks that learn to map visual observations and language instructions directly to robot actions. Unlike traditional robotic systems that rely on hand-engineered perception and control pipelines, VLA models learn end-to-end policies from demonstration data, enabling more flexible and generalizable robot behaviors.

These models typically:
- Process camera images to understand the scene
- Accept natural language commands to specify task goals
- Output low-level robot control commands (joint positions, velocities, etc.)

## Hardware Setup

The project uses the **SoArm-101** robot arm, a compact and accessible robotic manipulator suitable for tabletop manipulation tasks. The system is integrated with LeRobot, an open-source framework designed to facilitate robot learning research.

```mermaid
graph TB
    subgraph Hardware["Hardware Components"]
        ARM[SoArm-101 Robot Arm<br/>5-DOF + Gripper]
        CAM[Camera System<br/>Vision Sensors]
        CTRL[Teleoperation Interface<br/>Leader-Follower Control]
    end

    subgraph Software["Software Framework"]
        LR[LeRobot Framework]
        DC[Data Collection Pipeline]
        TR[Training Pipeline]
        DP[Deployment System]
    end

    ARM --> LR
    CAM --> LR
    CTRL --> LR
    LR --> DC
    LR --> TR
    LR --> DP

    style ARM fill:#e1f5ff
    style CAM fill:#e1f5ff
    style CTRL fill:#e1f5ff
    style LR fill:#fff4e1
```

Key components:
- **SoArm-101 Robot Arm**: 5-DOF robotic arm with gripper
- **Camera System**: Vision sensors for visual perception
- **Teleoperation Interface**: Custom control interface for collecting demonstration data
- **LeRobot Framework**: Provides tools for data collection, training, and deployment

# Project Phases

## Phase 1: Teleoperation & Data Collection Pipeline ✓ COMPLETE

The first phase focused on establishing a reliable data collection infrastructure. High-quality demonstration data is crucial for training effective VLA models. **This phase has been successfully completed.**

### Completed Tasks:
- **LeRobot Environment Setup**: Configured LeRobot framework with all necessary dependencies
- **Hardware Integration**: Successfully integrated SoArm-101 with LeRobot's control interfaces
- **Teleoperation Interface**: Developed an intuitive leader-follower teleoperation system where movements on a leader arm transmit to a follower arm in real-time
- **Data Collection Scripts**: Implemented automated scripts (`data_collection.py`) for recording and storing demonstration trajectories at 30 Hz
- **Sample Demonstrations**: Tested the complete pipeline by recording initial demonstration datasets
- **Camera & Sensor Pipeline**: Completed vision system configuration and integration
- **Data Format Standardization**: Implemented consistent data structures compatible with LeRobot's training format
- **Dataset Management**: Created utilities for organizing, versioning, and managing collected datasets with Hugging Face Hub integration

## Phase 2: Training & Deployment Pipeline ⚙️ IN PROGRESS

The second phase involves implementing and evaluating different VLA model architectures. **Significant progress has been made with ACT model implementation complete.**

### Model Architectures:

#### ✓ ACT (Action Chunking Transformer) - IMPLEMENTED
A transformer-based model that predicts sequences of future actions (action chunks) rather than single actions, enabling more coherent and long-horizon behaviors. The ACT training pipeline is now fully operational with:
- Comprehensive hyperparameter control system
- Weights & Biases integration for experiment tracking
- GPU-accelerated training with distributed computing support
- Configurable training parameters via command-line interface

#### ⧖ π0 (Pi-Zero) - PLANNED
A foundational VLA model architecture that learns visuomotor policies from demonstration data.

#### ⧖ π0.5 (Pi-Zero-Point-Five) - PLANNED
An enhanced version of π0 with improved architecture and training strategies.

### Completed Training Pipeline Components:
- ✓ GPU-accelerated training environment
- ✓ Comprehensive hyperparameter configuration management
- ✓ Model checkpointing and versioning
- ✓ Training visualization with Weights & Biases
- ⧖ Evaluation metrics and benchmarking scripts (in progress)

### Deployment Pipeline:
- ✓ Deployment automation scripts for Jetson devices
- ✓ Udev rules configuration for consistent hardware device naming
- ⧖ Model inference optimization for real-time control (in progress)
- ⧖ Safety monitoring and intervention systems (planned)
- ⧖ Performance evaluation on real hardware (planned)

# Technical Implementation

## LeRobot Integration

LeRobot provides a standardized interface for robot learning research. The integration with SoArm-101 required:

1. **Robot Driver Development**: Custom drivers for SoArm-101 hardware communication
2. **Calibration Procedures**: Kinematic calibration for accurate position control
3. **Data Recording Format**: Standardized format compatible with LeRobot's training pipeline
4. **Real-time Control**: Low-latency control loops for smooth teleoperation and policy execution

## Data Collection Methodology

The teleoperation system enables human operators to:
- Control the robot arm through an intuitive interface
- Demonstrate desired manipulation behaviors
- Record synchronized data streams (images, robot states, actions)
- Annotate demonstrations with task labels and language descriptions

The collected datasets serve as training data for the VLA models, where the models learn to imitate the demonstrated behaviors.

```mermaid
sequenceDiagram
    participant Operator
    participant Leader Arm
    participant Control System
    participant Follower Arm
    participant Camera
    participant Data Storage

    Operator->>Leader Arm: Manipulate leader arm
    Leader Arm->>Control System: Send joint positions
    Control System->>Follower Arm: Transmit commands (30 Hz)
    Follower Arm->>Control System: Return state feedback
    Camera->>Control System: Stream visual observations

    alt Recording Mode Active
        Control System->>Data Storage: Save episode data
        Note over Data Storage: Images + States + Actions<br/>+ Language Annotations
    end

    Note over Operator,Data Storage: Press ENTER to toggle recording<br/>Press CTRL+C to finalize & upload
```

## Usage Workflows

The project provides two main operational modes:

### Teleoperation Mode

Running `python teleop.py` establishes a leader-follower control system:
- Movements on the leader arm are transmitted to the follower arm in real-time
- Serial port configuration and arm identifiers are customizable via command-line parameters
- Enables intuitive control for demonstrating manipulation behaviors

### Data Collection Mode

The `data_collection.py` script captures demonstration episodes:
- Records synchronized data streams at 30 Hz (images, robot states, actions)
- Press ENTER to toggle recording on/off
- Press CTRL+C to finalize the session and upload datasets to Hugging Face Hub
- Datasets are automatically formatted for compatibility with LeRobot's training pipeline

## Model Training Workflow

The planned training workflow follows these steps:

1. **Data Preprocessing**: Normalize images and actions, augment data for robustness
2. **Model Architecture**: Implement the selected VLA model architecture
3. **Training Loop**: Optimize model parameters using collected demonstrations
4. **Validation**: Evaluate model performance on held-out demonstrations
5. **Deployment Testing**: Test trained policies on the physical robot
6. **Iteration**: Refine based on real-world performance

```mermaid
flowchart TD
    START([Start Training Pipeline]) --> LOAD[Load Demonstration Dataset]
    LOAD --> PREPROC[Data Preprocessing<br/>Normalize & Augment]
    PREPROC --> SPLIT{Split Dataset}

    SPLIT -->|80%| TRAIN_DATA[Training Set]
    SPLIT -->|20%| VAL_DATA[Validation Set]

    TRAIN_DATA --> MODEL[Select VLA Architecture<br/>π0 / π0.5 / ACT]
    MODEL --> TRAIN_LOOP[Training Loop<br/>Optimize Parameters]

    TRAIN_LOOP --> VAL{Validation<br/>Performance}
    VAL -->|Poor| HYPER[Adjust Hyperparameters]
    HYPER --> TRAIN_LOOP

    VAL -->|Good| CHECKPOINT[Save Model Checkpoint]
    CHECKPOINT --> DEPLOY[Deploy to Robot]
    DEPLOY --> TEST[Real-World Testing]

    TEST --> EVAL{Performance<br/>Acceptable?}
    EVAL -->|No| COLLECT[Collect More Data]
    COLLECT --> LOAD
    EVAL -->|Yes| END([Model Ready])

    style START fill:#d4edda
    style END fill:#d4edda
    style MODEL fill:#fff4e1
    style DEPLOY fill:#e1f5ff
```

# Current Status

The project has successfully completed Phase 1 and made significant progress in Phase 2:

**Phase 1 - Complete ✓**
- Teleoperating the SoArm-101 with low latency and configurable control frequencies
- Recording demonstration data in LeRobot-compatible format
- Managing collected datasets with automatic Hugging Face Hub integration

**Phase 2 - In Progress ⚙️**
- ACT (Action Chunking Transformer) training pipeline fully implemented
- Deployment automation for Jetson devices operational
- Training infrastructure established with GPU acceleration and experiment tracking
- Working on evaluation metrics and real-world deployment testing

The system now provides three main operational modes:
1. **Teleoperation Mode**: Real-time leader-follower control
2. **Data Collection Mode**: Keyboard-controlled episode recording with cloud upload
3. **Training Mode**: ACT model training with comprehensive hyperparameter control

# Future Directions

This project aims to:

1. **Benchmark Multiple VLA Architectures**: Compare π0, π0.5, ACT, and potentially other models on standardized tasks
2. **Dataset Expansion**: Collect diverse demonstration datasets covering various manipulation scenarios
3. **Performance Analysis**: Document detailed performance metrics, failure modes, and lessons learned
4. **Open-Source Contributions**: Share trained models, datasets, and implementation insights with the robotics community

# Technical Stack

- **Programming Language**: Python 3.12+
- **Dependency Management**: UV
- **Robotics Framework**: ROS2
- **Robot Learning Framework**: LeRobot (v0.4.2+)
- **Hardware Interface**: Feetech servos
- **Deep Learning**: PyTorch (via LeRobot)
- **Dataset Management**: Hugging Face Hub
- **Version Control**: Git

# Conclusion

This project represents an exploration of cutting-edge VLA models applied to practical robotic manipulation. By combining teleoperation, imitation learning, and state-of-the-art neural architectures, the goal is to develop capable and generalizable robot policies that can learn from demonstration and adapt to new scenarios.

The integration of LeRobot with SoArm-101 provides an accessible platform for robot learning research, and the insights gained from training and evaluating multiple VLA models will contribute to understanding the capabilities and limitations of these emerging approaches.

# License

This project is released under the MIT License, enabling broad usage and modification with proper attribution. The open-source nature of the project encourages collaboration and knowledge sharing within the robotics and machine learning communities.

# References

1. LeRobot: An Open-Source Framework for Robot Learning - [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)
2. Action Chunking Transformer (ACT) - Learning Fine-Grained Bimanual Manipulation
3. Vision-Language-Action Models for Robot Learning
