---
layout: post
title: "AI-Powered Autunomous Tangram Puzzle Solver"
categories: [C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics]
image: assets/images/tangram/tangram.gif
featured: true
hidden: true
---

C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics

**Source Code**: The source code for this project can be found here: [GitHub](https://github.com/nu-jliu/Autonomous_Tangram_Solver)

<!-- # Final Video
*Final Video Goes Here* -->

<iframe width="800" height="560" src="https://www.youtube.com/embed/1sL6v5JUFx0?si=-zl5taqeXtQbnnAh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# Objective
The objective of this project is to design and implement a robotic system that is able to solve the tangram puzzle given by the user. This system will use an advanced algorithms to sense, plan and control the robot arm to accurately assemble tangram pieces to match the target tangram puzzle configuraiton. This project aims to demostrate the capability of the robotic system to solve complex tasks by leverging computer vision, machine learning, artifical intelligence and robot manipulation.


# Hardware Setup
The hardware setup is shown in the picture below. This system consists of a robot arm and two cameras, which includes one overhead camera and one :

 - Robot Arm: To manipulate the tangram pieces to solve the tangram puzzle.
 - Overhead Camera: To detect the poses of tangram pieces. 
 - Front Camera: To detect the outline of the puzzle to solve.

![](/assets/images/tangram/setup_draw.png)

## Hand-Eye Calibration
The setup for performing hand-eye calibration is by putting a `apriltag` in the workspace of the robot arm as shown in the fugure below. The mechanism for calibration is to align the end-effector of the robot arm with the apriltag. By performing the calculation of homogeneous transformation, the transform between the robot frame to camera frame would be calculated.

Denote $r$ as robot, $c$ as camera and $t$ for tag, then the transformation between robot and camera $T_{rc}$ can be calculated as:



$$
\begin{aligned}
    T_{rc} &= T_{rt} \cdot T_{tc} \\
    &= T_{rt} \cdot T_{ct}^{-1}
\end{aligned}
$$

![](/assets/images/tangram/calibrate_arm.JPG)

![](/assets/images/tangram/calibration.png)

# System Workflow

![](/assets/images/tangram/workflow.png)

# Software Architecture

## Tangram Puzzle Detection

<!-- For puzzle detection, I trained a `YOLOv11` object detection model to detect the shape drawn in the whiteboard, and then find the corresponding image in the database. After getting the desired shape image, It will be fed into the model for solving tangram puzzle. -->

For puzzle detection, I trained a **YOLOv11** object detection model to identify shapes drawn on a whiteboard. Once a shape is detected, the system retrieves the corresponding image from the database. The selected shape image is then fed into the tangram puzzle-solving model for further processing and solution generation.

<!-- ![](/assets/images/tangram/yolo_detection.png) -->

## Tangram Puzzle Solver

<!-- For solving the tangram puzzle, I refered the paper [*Generative approaches for solving tangram puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6), in which they have designed a `Convolutional Autoencoder (CAE)` for solving tangram puzzles. The input and output for this system is shown in the figure below. The input will be the tangram puzzle, and the output will be the segmented shapes for solving the puzzle.  -->

To tackle the challenge of solving tangram puzzles, I leveraged insights from the paper [*Generative Approaches for Solving Tangram Puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6). The paper introduces a solution using a **Convolutional Autoencoder (CAE)** designed specifically for tangram puzzle segmentation.

The system processes the puzzle as its input and generates segmented shapes as its output, enabling a step-by-step solution to the puzzle. The figure below illustrates the input-output relationship, highlighting the CAE's capability to effectively decompose the puzzle into its constituent shapes.

![](/assets/images/tangram/tangram_solve.png)

<!-- The architecture for the network is shown in the figure below

![](/assets/images/tangram/tangram_cae.png) -->

The training loss is shown in the figure below:

![](/assets/images/tangram/epoch_loss_cae.png)

<!-- ### Target Configuration Generator -->

### Target Pose Detection

![](/assets//images/tangram/puzzle_seg.png)

![](/assets//images/tangram/puzzle_label.png)


## Tangram Piece Detection
<!-- To determine where the tangram pieces are for robot arm to pick them up, I used the sematic segmentation and morphological transformation to determine the shape and pose for each tangram piece. -->

To enable the robot arm to accurately pick up tangram pieces, I implemented **semantic segmentation** combined with **morphological transformations**. This approach effectively identifies the shape and pose of each tangram piece, ensuring precise localization for robotic manipulation.

### Tangram Pose Detection
#### Segmentation
![](/assets/images/tangram/piece_seg.png)

#### Morphological Transformation
![](/assets/images/tangram/piece_mask.png)

#### Shape Matching
![](/assets/images/tangram/piece_label.png)

### Frame Transformation



![](/assets/images/tangram/detection.png)

## Robot Action Planner

<!-- After getting all poses for tangram pieces and their target pose, the robot action can be generated to command the robot to pick up the tangram pieces and place it in their correspoding target pose. So that robot would be able to assemble the tangram pieces into a complete puzzle. -->

<!-- After determining the poses of the tangram pieces and their corresponding target positions, I generated robotic actions to guide the robot arm. These commands enabled the robot to pick up the tangram pieces and accurately place them in their designated target poses, successfully assembling the tangram puzzle into a complete configuration. -->

Once the poses of the tangram pieces and their corresponding target positions were determined, I developed a detailed sequence of robotic actions to facilitate the assembly process. 

The workflow involved the following steps:
1. **Pose Mapping**: Using the computed current and target poses, the system calculated the optimal path for each piece.
2. **Pick-and-Place Strategy**: The robot arm was programmed to execute precise pick-and-place actions. This involved:
   - Moving to the detected position of each tangram piece.
   - Adjusting the gripper for secure grasping based on the shape and orientation of the piece.
   - Lifting the piece without disturbing adjacent ones.
3. **Path Planning**: Trajectories were generated to ensure smooth and collision-free motion from the current pose to the target pose.
4. **Placement Accuracy**: At the target position, the robot arm adjusted the piece’s orientation to match the desired alignment and gently placed it into position.

This process enabled the robot to systematically pick up each tangram piece and assemble them into a complete puzzle with high precision and efficiency.

## Robot Controller

<!-- Since the robot arm is made of 4 servo motors, that its motion is controlled by the angle of rotation for each servo motor. To accurately control the robot to move to the given x, y, z cartesian coordinate, I implemented the forward and inverse kinematics for the robot arm to precisely control its motion in cartesian space. -->

The robot arm, powered by four servo motors, is operated through precise angle adjustments for each motor. To enable accurate movement to specified x, y, z Cartesian coordinates, I implemented **forward and inverse kinematics**. This ensured precise motion control and positioning of the robot arm within the Cartesian space.

### Kinematics

<!-- To implement the forward and inverse kinematics for the robot arm, I refer the [Modern Robotics](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf) to model and implement kinematics algorithm for the robot arm.  -->

To develop the forward and inverse kinematics for the robotic arm, I referred to the textbook [*Modern Robotics*](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf). Using this resource, I modeled and implemented the kinematics algorithms required for precise control of the robot arm's motion.

#### Robot Modeling

<!-- As illustrated in the figure below, the robot arm is a 4-DOF robot arm, with 4-bar linkage so that the end-effector is always pointing downward.  -->

As shown in the figure below, the robot arm is a **4-DOF (Degrees of Freedom)** system featuring a **4-bar linkage mechanism**. This design ensures that the end-effector consistently maintains a downward orientation during operation.

<!-- To take account in the 4-bar linkage, I created a fake joint as shown in the figure to correctly represent the kinematics of the robot arm. -->

To accurately model the kinematics of the robot arm with its **4-bar linkage**, I introduced a **virtual joint** (illustrated in the figure). This adjustment allowed for precise representation and computation of the robot's motion dynamics.

The home configuration can be denoted as 

$$
M = \begin{bmatrix}
    1 & 0 & 0 & L_3 \\
    0 & -1 & 0 & 0 \\
    0 & 0 & -1 & L_1 + L_2 - L_4 \\
    0 & 0 & 0 & 1
\end{bmatrix}
$$

And the ${\cal S}_{list}$ and ${\cal B}_{list}$ can be represented as 

$$
\begin{align*}
S_{list} &= \begin{bmatrix}
    0 & 1 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & -1 \\
    0 & 0 & 0 & 0 & L_3\\
    0 & L_1 & L_1 + L_2 & L_1 + L_2 & 0\\
    0 & 0 & 0 & -L_3 & 0
\end{bmatrix} \\
B_{list} &= \begin{bmatrix}
    0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 1 & 0 \\
    -1 & 0 & 0 & 0 & 1 \\
    0 & L_4 - L_2 & 0 & L_4 & 0 \\
    0 & 0 & 0 & 0 & 0\\
    0 & -L_3 & -L_3 & 0 & 0
\end{bmatrix} \\
\end{align*}
$$


#### Forward and Inverse Kinematics

After finishing modeling the robot, we could apply the equations described in book [*Modern Robotics*](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf) for calculating the forward and inverse kinematics

$$
T_{sb}(\theta) = Me^{[{\cal B_1}]\theta_1}e^{[{\cal B_2}]\theta_2}e^{[{\cal B_3}]\theta_3}e^{[{\cal B_4}]\theta_4}e^{[{\cal B_5}]\theta_5}
$$

# Result

# Future Work

# References: 

1. Fernanda Miyuki Yamada, Harlen Costa Batagelo, João Paulo Gois, and H. Takahashi, “Generative approaches for solving tangram puzzles,” Discover Artificial Intelligence, vol. 4, no. 1, Feb. 2024, doi: https://doi.org/10.1007/s44163-024-00107-6.

2. K. M. Lynch and F. C. Park, Modern robotics : mechanics, planning, and control. Cambridge: University Press, 2017.
‌
‌