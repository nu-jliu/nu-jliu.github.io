---
layout: post
title: "Autunomous Tangram Puzzle Solver"
categories: [C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics]
image: assets/images/tangram/tangram.gif
featured: true
hidden: true
---

C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics

**Source Code**: The source code for this project can be found here: [GitHub](https://github.com/nu-jliu/Autonomous_Tangram_Solver)

<!-- # Final Video
*Final Video Goes Here* -->

<iframe width="560" height="315" src="https://www.youtube.com/embed/1sL6v5JUFx0?si=-zl5taqeXtQbnnAh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# Objective
The objective of this project is to design and implement a robotic system that is able to solve the tangram puzzle given by the user. This system will use an advanced algorithms to sense, plan and control the robot arm to accurately assemble tangram pieces to match the target tangram puzzle configuraiton. This project aims to demostrate the capability of the robotic system to solve complex tasks by leverging computer vision, machine learning, artifical intelligence and robot manipulation.


# Hardware Setup
The hardware setup is shown in the picture below. This system consists of a robot arm and two cameras, which includes one overhead camera and one :

 - Robot Arm: To manipulate the tangram pieces to solve the tangram puzzle.
 - Overhead Camera: To detect the poses of tangram pieces. 
 - Front Camera: To detect the outline of the puzzle to solve.

![](/assets/images/tangram/setup.JPG)

## Hand-Eye Calibration
The setup for performing hand-eye calibration is by putting a `apriltag` in the workspace of the robot arm as shown in the fugure below. The mechanism for calibration is to align the end-effector of the robot arm with the apriltag. By performing the calculation of homogeneous transformation, the transform between the robot frame to camera frame would be calculated.

<!-- Denote $r$ for robot, $c$ for camera and $t$ for tag -->



$$
\begin{aligned}
    T_{rc} &= T_{rt} \cdot T_{tc} \\
    &= T_{rt} \cdot T_{ct}^{-1}
\end{aligned}
$$

![](/assets/images/tangram/calibrate_arm.JPG)

![](/assets/images/tangram/calibration.png)

# System Workflow

![](/assets/images/tangram/workflow.png)

# Software Architecture

## Tangram Puzzle Detection

<!-- For puzzle detection, I trained a `YOLOv11` object detection model to detect the shape drawn in the whiteboard, and then find the corresponding image in the database. After getting the desired shape image, It will be fed into the model for solving tangram puzzle. -->

For puzzle detection, I trained a **YOLOv11** object detection model to identify shapes drawn on a whiteboard. Once a shape is detected, the system retrieves the corresponding image from the database. The selected shape image is then fed into the tangram puzzle-solving model for further processing and solution generation.

<!-- ![](/assets/images/tangram/yolo_detection.png) -->

## Tangram Puzzle Solver

<!-- For solving the tangram puzzle, I refered the paper [*Generative approaches for solving tangram puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6), in which they have designed a `Convolutional Autoencoder (CAE)` for solving tangram puzzles. The input and output for this system is shown in the figure below. The input will be the tangram puzzle, and the output will be the segmented shapes for solving the puzzle.  -->

To tackle the challenge of solving tangram puzzles, I leveraged insights from the paper [*Generative Approaches for Solving Tangram Puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6). The paper introduces a solution using a **Convolutional Autoencoder (CAE)** designed specifically for tangram puzzle segmentation.

The system processes the puzzle as its input and generates segmented shapes as its output, enabling a step-by-step solution to the puzzle. The figure below illustrates the input-output relationship, highlighting the CAE's capability to effectively decompose the puzzle into its constituent shapes.

![](/assets/images/tangram/tangram_solve.png)

<!-- The architecture for the network is shown in the figure below

![](/assets/images/tangram/tangram_cae.png) -->

The training loss is shown in the figure below:

![](/assets/images/tangram/epoch_loss_cae.png)

<!-- ### Target Configuration Generator -->

### Target Pose Detection

![](/assets//images/tangram/puzzle_seg.png)

![](/assets//images/tangram/puzzle_label.png)


## Tangram Piece Detection
<!-- To determine where the tangram pieces are for robot arm to pick them up, I used the sematic segmentation and morphological transformation to determine the shape and pose for each tangram piece. -->

To enable the robot arm to accurately pick up tangram pieces, I implemented **semantic segmentation** combined with **morphological transformations**. This approach effectively identifies the shape and pose of each tangram piece, ensuring precise localization for robotic manipulation.

### Tangram Pose Detection
#### Segmentation
![](/assets/images/tangram/piece_seg.png)

#### Morphological Transformation
![](/assets/images/tangram/piece_mask.png)

#### Shape Matching
![](/assets/images/tangram/piece_label.png)

### Frame Transformation



![](/assets/images/tangram/detection.png)

## Robot Action Planner

## Robot Controller

# Result

# Future Work