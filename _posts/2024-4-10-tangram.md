---
layout: post
title: "AI-Powered Autunomous Tangram Puzzle Solver"
categories: [C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics]
image: assets/images/tangram/tangram.gif
featured: true
hidden: true
---

C++, Python, ROS2, YOLO, Convolutional Autoencoder, Sematic Segmentation, Computer Vision, Machine Learning, Robot Kinematics

**Source Code**: The source code for this project can be found here: [GitHub](https://github.com/nu-jliu/Autonomous_Tangram_Solver)

<!-- # Final Video
*Final Video Goes Here* -->

<iframe width="800" height="560" src="https://www.youtube.com/embed/1sL6v5JUFx0?si=-zl5taqeXtQbnnAh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# Objective
<!-- The program of solving tangram puzzle is known to be **NP**-hard, it would be challenging for a robot to be able to solve the tangram puzzle. The objective of this project is to design and implement a robotic system that is able to solve the tangram puzzle given by the user. This system will use an advanced algorithms to sense, plan and control the robot arm to accurately assemble tangram pieces to match the target tangram puzzle configuraiton. This project aims to demostrate the capability of the robotic system to solve complex tasks by leverging computer vision, machine learning, artifical intelligence and robot manipulation. -->


The objective of this project is to design and implement a robotic system capable of solving tangram puzzles specified by the user. Tangram puzzles are **NP-hard** problems, presenting a significant challenge for robotic systems to solve. This project aims to develop a system that integrates advanced algorithms to sense, plan, and control a robotic arm, enabling it to accurately assemble tangram pieces to match the target puzzle configuration. By leveraging cutting-edge technologies such as **computer vision**, **machine learning**, **artificial intelligence**, and **robotic manipulation**, this project seeks to demonstrate the capability of robotic systems to perform complex tasks with precision and efficiency.

# Hardware Setup
The hardware setup is shown in the picture below. This system consists of a robot arm and two cameras, which includes one overhead camera and one :

 - Robot Arm: To manipulate the tangram pieces to solve the tangram puzzle.
 - Overhead Camera: To detect the poses of tangram pieces. 
 - Front Camera: To detect the outline of the puzzle to solve.

![](/assets/images/tangram/setup_draw.png)

## Hand-Eye Calibration
The setup for performing hand-eye calibration is by putting a `apriltag` in the workspace of the robot arm as shown in the fugure below. The mechanism for calibration is to align the end-effector of the robot arm with the apriltag. By performing the calculation of homogeneous transformation, the transform between the robot frame to camera frame would be calculated.

Denote $r$ as robot, $c$ as camera and $t$ for tag, then the transformation between robot and camera $T_{rc}$ can be calculated as:



$$
\begin{aligned}
    T_{rc} &= T_{rt} \cdot T_{tc} \\
    &= T_{rt} \cdot T_{ct}^{-1}
\end{aligned}
$$

![](/assets/images/tangram/calibrate_arm.JPG)

![](/assets/images/tangram/calibration.png)

# System Workflow

![](/assets/images/tangram/workflow.png)

# Software Architecture

## Tangram Puzzle Detection

<!-- For puzzle detection, I trained a `YOLOv11` object detection model to detect the shape drawn in the whiteboard, and then find the corresponding image in the database. After getting the desired shape image, It will be fed into the model for solving tangram puzzle. -->

For puzzle detection, I trained a **YOLOv11** object detection model to identify shapes drawn on a whiteboard. Once a shape is detected, the system retrieves the corresponding image from the database. The selected shape image is then fed into the tangram puzzle-solving model for further processing and solution generation.

<!-- ![](/assets/images/tangram/yolo_detection.png) -->

## Tangram Puzzle Solver

<!-- For solving the tangram puzzle, I refered the paper [*Generative approaches for solving tangram puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6), in which they have designed a `Convolutional Autoencoder (CAE)` for solving tangram puzzles. The input and output for this system is shown in the figure below. The input will be the tangram puzzle, and the output will be the segmented shapes for solving the puzzle.  -->

To tackle the challenge of solving tangram puzzles, I leveraged insights from the paper [*Generative Approaches for Solving Tangram Puzzles*](https://link.springer.com/article/10.1007/s44163-024-00107-6). The paper introduces a solution using a **Convolutional Autoencoder (CAE)** designed specifically for tangram puzzle segmentation.

The system processes the puzzle as its input and generates segmented shapes as its output, enabling a step-by-step solution to the puzzle. The figure below illustrates the input-output relationship, highlighting the CAE's capability to effectively decompose the puzzle into its constituent shapes.

![](/assets/images/tangram/tangram_solve.png)

<!-- The architecture for the network is shown in the figure below

![](/assets/images/tangram/tangram_cae.png) -->

The training loss is shown in the figure below:

![](/assets/images/tangram/epoch_loss_cae.png)

<!-- ### Target Configuration Generator -->

### Target Pose Detection

<!-- ![](/assets//images/tangram/puzzle_seg.png)

![](/assets//images/tangram/puzzle_label.png) -->


![](/assets/images/tangram/puzzle_solver.png)

<!-- The workflow to determine the configuration of each tangram pieces in robot frame is shown in the diagram above, the entire process is divided into the following steps:

1. *Model Prediction*: Segment the puzzle into tangram pieces to get the target configuration.
2. *Erosion*: Erodes the boundary of each mask to prevent two adjacent tangram pieces being connected together.
3. *Opening*: Removing the white noices in the black background.
4. *Closing*: Removing the black noices in the tangram pieces.
5. *Find Contour*: Applied the `Canny` edge-detection algorithm to find the contour of each tangram piece.
6. *Approximate Polygon*: Fit the contour to the closest polygon to determine its shape and orientation. -->

The process to determine the configuration of each tangram piece in the robot frame is outlined in the diagram above. This workflow ensures accurate identification and localization of each tangram piece, enabling effective robotic manipulation. The process is divided into the following steps:

1. **Model Prediction (CAE)**: The puzzle image is processed using a trained **Convolutional Autoencoder (CAE)** to segment the tangram puzzle into individual pieces and identify their target configurations.
2. **Erosion**: The boundaries of each segmented mask are eroded to prevent adjacent tangram pieces from being erroneously connected.
3. **Opening**: Morphological opening is applied to remove white noise from the black background, ensuring clean segmentation of the puzzle pieces.
4. **Closing**: Morphological closing is used to eliminate black noise within the tangram pieces, preserving their integrity.
5. **Find Contour**: The `Canny` edge-detection algorithm is employed to locate the contours of each tangram piece, isolating their edges.
6. **Approximate Polygon**: The detected contours are approximated to the closest polygon, enabling precise determination of each tangram piece’s shape and orientation.

This comprehensive approach leverages the CAE model and robust image processing techniques to ensure tangram pieces are accurately configured within the robot's frame, facilitating precise and efficient robotic operations.

## Tangram Piece Detection
<!-- To determine where the tangram pieces are for robot arm to pick them up, I used the sematic segmentation and morphological transformation to determine the shape and pose for each tangram piece. -->

To enable the robot arm to accurately pick up tangram pieces, I implemented **semantic segmentation** combined with **morphological transformations**. This approach effectively identifies the shape and pose of each tangram piece, ensuring precise localization for robotic manipulation.

### Tangram Pose Detection

![](/assets/images/tangram/piece_detect.png)

<!-- The workflow of detecting the pose of tangram includes following steps:

1. *Sematic Segmentation*: Segment the input image into tangram pieces.
2. "Mask Generation": Generate a mask from the segmented image 
3. *Computer Vision*: Use the computer vision algorithm to get pose as expressed in x, y pixel coordinates and shape for each tangram piece.
4. *Deprojection*: Deproject from the x, y pixel coordinate to x, y, z cartesian coordinate in camera frame. -->

As illustrated in the figure above, the workflow for detecting the pose of tangram pieces includes the following steps:

1. **Semantic Segmentation**: Segment the input image to identify individual tangram pieces.
2. **Mask Generation**: Generate a mask from the segmented image to isolate each tangram piece.
3. **Edge Detection**: Apply edge-detection algorithms to identify the contours of the tangram pieces from the masked image.
4. **Contour Analysis**: Analyze each detected contour to determine its centroid, shape, and orientation.
5. **Deprojection**: Transform the $ x, y $ pixel coordinates into $ x, y, z $ Cartesian coordinates within the camera frame, enabling precise 3D localization of the tangram pieces.

### Frame Transformation

From the **Hand-eye Calibration** step, the transformation matrix $ T_{rc} $ (robot frame to camera frame) was obtained. Using this transformation, the position of each tangram piece in the camera frame ($ \vec{p}_c $) can be converted to the corresponding position in the robot frame ($ \vec{p}_r $).

The transformation is computed as follows:

$$
\vec{p}_r = T_{rc} \times \vec{p}_c
$$

This calculation enables accurate localization of tangram pieces in the robot's coordinate frame, ensuring precise robotic manipulation during the assembly process.

The resulting detected position of each tangram piece in the camera frame is shown in the figure below:

![](/assets/images/tangram/detection.png)

## Robot Action Planner

<!-- After getting all poses for tangram pieces and their target pose, the robot action can be generated to command the robot to pick up the tangram pieces and place it in their correspoding target pose. So that robot would be able to assemble the tangram pieces into a complete puzzle. -->

<!-- After determining the poses of the tangram pieces and their corresponding target positions, I generated robotic actions to guide the robot arm. These commands enabled the robot to pick up the tangram pieces and accurately place them in their designated target poses, successfully assembling the tangram puzzle into a complete configuration. -->

Once the poses of the tangram pieces and their corresponding target positions were determined, I developed a detailed sequence of robotic actions to facilitate the assembly process. 

The workflow involved the following steps:
1. **Pose Mapping**: Using the computed current and target poses, the system calculated the optimal path for each piece.
2. **Pick-and-Place Strategy**: The robot arm was programmed to execute precise pick-and-place actions. This involved:
   - Moving to the detected position of each tangram piece.
   - Adjusting the gripper for secure grasping based on the shape and orientation of the piece.
   - Lifting the piece without disturbing adjacent ones.
3. **Path Planning**: Trajectories were generated to ensure smooth and collision-free motion from the current pose to the target pose.
4. **Placement Accuracy**: At the target position, the robot arm adjusted the piece’s orientation to match the desired alignment and gently placed it into position.

This process enabled the robot to systematically pick up each tangram piece and assemble them into a complete puzzle with high precision and efficiency.

## Robot Controller

<!-- Since the robot arm is made of 4 servo motors, that its motion is controlled by the angle of rotation for each servo motor. To accurately control the robot to move to the given x, y, z cartesian coordinate, I implemented the forward and inverse kinematics for the robot arm to precisely control its motion in cartesian space. -->

The robot arm, powered by four servo motors, is operated through precise angle adjustments for each motor. To enable accurate movement to specified x, y, z Cartesian coordinates, I implemented **forward and inverse kinematics**. This ensured precise motion control and positioning of the robot arm within the Cartesian space.

### Kinematics

<!-- To implement the forward and inverse kinematics for the robot arm, I refer the [Modern Robotics](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf) to model and implement kinematics algorithm for the robot arm.  -->

To develop the forward and inverse kinematics for the robotic arm, I referred to the textbook [*Modern Robotics*](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf). Using this resource, I modeled and implemented the kinematics algorithms required for precise control of the robot arm's motion.

#### Robot Modeling

<!-- As illustrated in the figure below, the robot arm is a 4-DOF robot arm, with 4-bar linkage so that the end-effector is always pointing downward.  -->

As shown in the figure below, the robot arm is a **4-DOF (Degrees of Freedom)** system featuring a **4-bar linkage mechanism**. This design ensures that the end-effector consistently maintains a downward orientation during operation.

![](/assets/images/tangram/kinematics.png)

<!-- To take account in the 4-bar linkage, I created a fake joint as shown in the figure to correctly represent the kinematics of the robot arm. -->

To accurately model the kinematics of the robot arm with its **4-bar linkage**, I introduced a **virtual joint** (illustrated in the figure). This adjustment allowed for precise representation and computation of the robot's motion dynamics.

The home configuration can be denoted as 

$$
M = \begin{bmatrix}
    1 & 0 & 0 & L_3 \\
    0 & -1 & 0 & 0 \\
    0 & 0 & -1 & L_1 + L_2 - L_4 \\
    0 & 0 & 0 & 1
\end{bmatrix}
$$

And the ${\cal S}_{list}$ and ${\cal B}_{list}$ can be represented as 

$$
\begin{align*}
S_{list} &= \begin{bmatrix}
    0 & 1 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & -1 \\
    0 & 0 & 0 & 0 & L_3\\
    0 & L_1 & L_1 + L_2 & L_1 + L_2 & 0\\
    0 & 0 & 0 & -L_3 & 0
\end{bmatrix} \\
B_{list} &= \begin{bmatrix}
    0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 1 & 0 \\
    -1 & 0 & 0 & 0 & 1 \\
    0 & L_4 - L_2 & 0 & L_4 & 0 \\
    0 & 0 & 0 & 0 & 0\\
    0 & -L_3 & -L_3 & 0 & 0
\end{bmatrix} \\
\end{align*}
$$


#### Forward and Inverse Kinematics

After finishing modeling the robot, we could apply the equations described in book [*Modern Robotics*](https://hades.mech.northwestern.edu/images/7/7f/MR.pdf) for calculating the forward and inverse kinematics
$$
T_{sb}(\theta) = Me^{[{\cal B_1}]\theta_1}e^{[{\cal B_2}]\theta_2}e^{[{\cal B_3}]\theta_3}e^{[{\cal B_4}]\theta_4}e^{[{\cal B_5}]\theta_5}
$$

# Result

The assembled tangram puzzle is displayed in the figure below, demonstrating the robotic system's ability to successfully complete a complex task. This result highlights the effectiveness of the system's integration of advanced algorithms for sensing, planning, and manipulation in achieving precise assembly of the puzzle.

![](/assets/images/tangram/final.JPG)

# Future Work
There are some components in the project that requires improvements. 

1. **Robot Control**: Due to the hardware limitation, the robot actuators are not precise enough for accurately grasp all tangram pieces at all time, so in the future, I would implememt a close-loop robot control loop to use the apriltag mounted in the robot arm as feedback to 

# References: 

1. Fernanda Miyuki Yamada, Harlen Costa Batagelo, João Paulo Gois, and H. Takahashi, “Generative approaches for solving tangram puzzles,” Discover Artificial Intelligence, vol. 4, no. 1, Feb. 2024, doi: https://doi.org/10.1007/s44163-024-00107-6.

2. K. M. Lynch and F. C. Park, Modern robotics : mechanics, planning, and control. Cambridge: University Press, 2017.
‌
‌